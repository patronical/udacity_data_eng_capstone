{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Tweets Worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores=mp.cpu_count()\n",
    "print(\"Number of processors: \", cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "df_raw = pd.DataFrame()\n",
    "\n",
    "root = 'raw_data'\n",
    "for y in tqdm_notebook(range(2008,2020)):\n",
    "    for m in range(1,13):\n",
    "        \n",
    "        filename = 'raw_tweets_{}_{}.pkl'.format(y,m)\n",
    "        filestring = os.path.join(root, filename)\n",
    "        \n",
    "        with open(filestring, 'rb') as filehandle:  \n",
    "            # read the data as binary data stream\n",
    "            df_slice = pickle.load(filehandle)\n",
    "            \n",
    "        df_raw = pd.concat([df_raw, df_slice], ignore_index=True)\n",
    "\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user_id feature\n",
    "The twitter screen_name has an associated twitter account number.  \n",
    "The account number is numerical representation of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the seed list put it into a dictionary for faster lookups\n",
    "df_seeds = pd.read_csv('seeds_061419.csv')\n",
    "\n",
    "# strip the spaces\n",
    "screens = [k.strip() for k in df_seeds['screen_name']]\n",
    "df_seeds['screen_name'] = screens\n",
    "\n",
    "#convert to dictionary for faster lookup\n",
    "lookup = df_seeds.set_index('screen_name').T.to_dict('series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find user_id's\n",
    "df_prc = pd.DataFrame()\n",
    "uids = []\n",
    "tids = []\n",
    "for _,nrows in tqdm_notebook(df_raw.iterrows()):\n",
    "    tids.append(nrows['tweet_id'])\n",
    "    user = np.asscalar(lookup[nrows['screen_name'].strip()])\n",
    "    uids.append(user)\n",
    "\n",
    "df_prc['tweet_id'] = tids\n",
    "df_prc['user_id'] = uids\n",
    "\n",
    "df_prc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle as a precaution\n",
    "with open('tweet_user_ids.pkl', 'wb') as filehandle:  \n",
    "    # store the data\n",
    "    pickle.dump(df_prc, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Language\n",
    "The sentiment converter likely works best in English.  \n",
    "Other languages may inject noise into the sentiment conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import exported function for multiprocessing\n",
    "import langdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split giant dataframe into small pieces\n",
    "df_tw = df_raw.copy()\n",
    "df_split = np.array_split(df_tw, 64, axis=0)\n",
    "len(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect languages and store\n",
    "root = 'proc_data'\n",
    "for i in tqdm_notebook(range(64)):\n",
    "    \n",
    "    df_proc = df_split[i]\n",
    "    \n",
    "    # create the multiprocessing pool\n",
    "    pool = mp.Pool(cores)\n",
    "\n",
    "    # process the tweet text\n",
    "    with mp.Pool(cores) as pool:\n",
    "        result = pool.map(langdet.langfind, df_proc['text'].iteritems())\n",
    "\n",
    "    # close down the pool\n",
    "    pool.close()\n",
    "    \n",
    "    # write result to column\n",
    "    df_proc['lang'] = result\n",
    "    \n",
    "    filename = 'proc_lang_{}.pkl'.format(i)\n",
    "    filestring = os.path.join(root, filename)\n",
    "    \n",
    "    # output to pickle\n",
    "    with open(filestring, 'wb') as filehandle:  \n",
    "        # store the data\n",
    "        pickle.dump(df_proc, filehandle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Tweets\n",
    "Some attempt to improve the language prior to sentiment conversion is made.\n",
    "This effort is intended to reduce the sentiment noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    # take out space after @ symbol\n",
    "    clear = re.sub(r'@\\s+',r'@', text)\n",
    "    # take out usernames\n",
    "    clear = re.sub(r'@([A-Za-z0-9_]+)','', clear)\n",
    "    # take out space after # symbol\n",
    "    clear = re.sub(r'#\\s+',r'#', clear)\n",
    "    # take out space after $ symbol\n",
    "    clear = re.sub(r'$\\s+',r'$', clear)\n",
    "    # take out url\n",
    "    clear = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', clear) \n",
    "    # take out pics\n",
    "    clear = re.sub(r'pic.twitter?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', clear)\n",
    "    # take out the extra white spaces\n",
    "    clear = re.sub(r' +', ' ', clear.strip())\n",
    "    return clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the cleaner\n",
    "cleaner('@ threadreaderapp unraveled  message https://www.google.co.uk/amp/s/ ...pic.twitter.com/Hoqk6XMkgn # CourageAward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the dataframes, tag the English entries\n",
    "for i in tqdm_notebook(range(64)):\n",
    "    \n",
    "    # input from pickle\n",
    "    filename_in = 'proc_lang_{}.pkl'.format(i)\n",
    "    filestring_in = os.path.join(root, filename_in)\n",
    "    \n",
    "    with open(filestring_in, 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "        df_eng = pickle.load(filehandle)\n",
    "        \n",
    "    # clean each text block\n",
    "    english = []\n",
    "    length = []\n",
    "    cleantext = []\n",
    "    for j,nrows in tqdm_notebook(df_eng.iterrows()):\n",
    "        tag = 0\n",
    "        if nrows['lang'] == 'en':\n",
    "            tag = 1 \n",
    "        clean = cleaner(nrows['text'])\n",
    "        length.append(len(clean))\n",
    "        cleantext.append(clean)\n",
    "        english.append(tag)\n",
    "    df_eng['english'] = english\n",
    "    df_eng['length'] = length\n",
    "    df_eng['cleantext'] = cleantext\n",
    "    \n",
    "    # output to pickle\n",
    "    filename_out = 'proc_clean_{}.pkl'.format(i)\n",
    "    filestring_out = os.path.join(root, filename_out)\n",
    "    \n",
    "    with open(filestring_out, 'wb') as filehandle:  \n",
    "        # store the data\n",
    "        pickle.dump(df_eng, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng['cleantext'].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the clean text\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "for i in tqdm_notebook(range(64)):\n",
    "    \n",
    "    # input from pickle\n",
    "    filename_in = 'proc_clean_{}.pkl'.format(i)\n",
    "    filestring_in = os.path.join(root, filename_in)\n",
    "    \n",
    "    with open(filestring_in, 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "        df_clean = pickle.load(filehandle)\n",
    "        \n",
    "    # tokenize through dataframe \n",
    "    tok_list = []\n",
    "    len_list = []\n",
    "\n",
    "    for _,nrows in tqdm_notebook(df_clean.iterrows()):\n",
    "        word_list = tknzr.tokenize(nrows['cleantext'])\n",
    "        tok_list.append(word_list)\n",
    "        len_list.append(len(word_list))\n",
    "                    \n",
    "    df_clean['tokens'] = tok_list\n",
    "    df_clean['tok_count'] = len_list\n",
    "        \n",
    "    # output to pickle\n",
    "    filename_out = 'proc_token_{}.pkl'.format(i)\n",
    "    filestring_out = os.path.join(root, filename_out)\n",
    "    \n",
    "    with open(filestring_out, 'wb') as filehandle:  \n",
    "        # store the data\n",
    "        pickle.dump(df_clean, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lowercase tokens and remove stop words for Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = list(set(stopwords.words('english'))) + \\\n",
    "            ['.',',','!','\"','…','?','’',':','-','/',')','(','$','...','&','*',':)','%','”','“',\"'\",'+',';']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from tokens and count keywords\n",
    "\n",
    "for i in tqdm_notebook(range(64)):\n",
    "    \n",
    "    # input from pickle\n",
    "    filename_in = 'proc_token_{}.pkl'.format(i)\n",
    "    filestring_in = os.path.join(root, filename_in)\n",
    "    \n",
    "    with open(filestring_in, 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "        df_token = pickle.load(filehandle)\n",
    "        \n",
    "    # clear the stopwords\n",
    "    bare_tok = []\n",
    "    bare_cnt = []\n",
    "    for _,nrows in tqdm_notebook(df_token.iterrows()):\n",
    "        bare = []\n",
    "        for Tok in nrows['tokens']:\n",
    "            tok = Tok.lower()\n",
    "            if not (tok in stoplist):\n",
    "                bare.append(tok)\n",
    "        bare_set = list(set(bare))\n",
    "        bare_tok.append(bare_set)\n",
    "        bare_cnt.append(len(bare_set))\n",
    "        \n",
    "    # add bare features to dataframe\n",
    "    df_token['keywords'] = bare_tok\n",
    "    df_token['key_count'] = bare_cnt\n",
    "        \n",
    "    # output to pickle\n",
    "    filename_out = 'proc_keyword_{}.pkl'.format(i)\n",
    "    filestring_out = os.path.join(root, filename_out)\n",
    "    \n",
    "    with open(filestring_out, 'wb') as filehandle:  \n",
    "        # store the data\n",
    "        pickle.dump(df_token, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tweet to sentiment\n",
    "\n",
    "for i in tqdm_notebook(range(64)):\n",
    "    \n",
    "    # input from pickle\n",
    "    filename_in = 'proc_keyword_{}.pkl'.format(i)\n",
    "    filestring_in = os.path.join(root, filename_in)\n",
    "    \n",
    "    with open(filestring_in, 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "        df_key = pickle.load(filehandle)\n",
    "        \n",
    "    # transform tweets to sentiment\n",
    "    pos_list = []\n",
    "    neu_list = []\n",
    "    neg_list = []\n",
    "    cmp_list = []\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for _,nrows in tqdm_notebook(df_key.iterrows()):\n",
    "        vs = analyzer.polarity_scores(nrows['cleantext'])\n",
    "        pos_list.append(vs['pos'])\n",
    "        neu_list.append(vs['neu'])\n",
    "        neg_list.append(vs['neg'])\n",
    "        cmp_list.append(vs['compound'])\n",
    "\n",
    "    df_key['vad_positive'] = pos_list\n",
    "    df_key['vad_neutral'] = neu_list\n",
    "    df_key['vad_negative'] = neg_list\n",
    "    df_key['vad_compound'] = cmp_list\n",
    "        \n",
    "    # output to pickle\n",
    "    filename_out = 'proc_sent_{}.pkl'.format(i)\n",
    "    filestring_out = os.path.join(root, filename_out)\n",
    "    \n",
    "    with open(filestring_out, 'wb') as filehandle:  \n",
    "        # store the data\n",
    "        pickle.dump(df_key, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag Outliers\n",
    "The English entries were tagged as 1's for good behavior.   \n",
    "By contrast the Outliers are tagged as 0's.  \n",
    "On filtering, good behavior filters are the 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load selected poets - found in analytics worksheet\n",
    "with open('screened_seeds_06119.pkl', 'rb') as filehandle:  \n",
    "    # store the data as binary data stream\n",
    "    selected = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword context checked against lower case keywords\n",
    "context = ['bitcoin', '#bitcoin', 'btc', 'bitcoins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Outlier Tags\n",
    "# tokenize the clean text\n",
    "tknzr = TweetTokenizer()\n",
    "for i in tqdm_notebook(range(64)):\n",
    "    \n",
    "    # input from pickle\n",
    "    filename_in = 'proc_sent_{}.pkl'.format(i)\n",
    "    filestring_in = os.path.join(root, filename_in)\n",
    "    \n",
    "    with open(filestring_in, 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "        df_sent = pickle.load(filehandle)\n",
    "        \n",
    "    # transform tweets to sentiment\n",
    "    poet_ok = []\n",
    "    tweet_ok = []\n",
    "    sent_ok = []\n",
    "    keyword_ok = []\n",
    "\n",
    "    for _,nrows in tqdm_notebook(df_sent.iterrows()):\n",
    "\n",
    "        # check poet\n",
    "        p_flag = 0\n",
    "        if nrows['screen_name'] in selected:\n",
    "            p_flag = 1\n",
    "        \n",
    "        #check tweet isn't short\n",
    "        t_flag = 0\n",
    "        if nrows['key_count'] > 3:\n",
    "            t_flag = 1\n",
    "        \n",
    "        #check sentiment\n",
    "        s_flag = 0\n",
    "        if ((nrows['vad_compound']!=0)&(nrows['vad_neutral']!=1)):\n",
    "            s_flag = 1\n",
    "        \n",
    "        #check keywords\n",
    "        k_flag = 0\n",
    "        hits = [k for k in nrows['keywords'] if k in context]\n",
    "        if len(hits)>0:\n",
    "            k_flag = 1\n",
    "        \n",
    "        poet_ok.append(p_flag)\n",
    "        tweet_ok.append(t_flag)\n",
    "        sent_ok.append(s_flag)\n",
    "        keyword_ok.append(k_flag)\n",
    "\n",
    "    df_sent['poet_ok'] = poet_ok\n",
    "    df_sent['tweet_ok'] = tweet_ok\n",
    "    df_sent['sent_ok'] = sent_ok\n",
    "    df_sent['key_ok'] = keyword_ok\n",
    "        \n",
    "    # output to pickle\n",
    "    filename_out = 'proc_out_{}.pkl'.format(i)\n",
    "    filestring_out = os.path.join(root, filename_out)\n",
    "    \n",
    "    with open(filestring_out, 'wb') as filehandle:  \n",
    "        # store the data\n",
    "        pickle.dump(df_sent, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build TSI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import processed dataframe\n",
    "df_processed = pd.DataFrame()\n",
    "for i in tqdm_notebook(range(64)):\n",
    "    \n",
    "    # input from pickle\n",
    "    filename_in = 'proc_out_{}.pkl'.format(i)\n",
    "    filestring_in = os.path.join(root, filename_in)\n",
    "    \n",
    "    with open(filestring_in, 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "        df_slice = pickle.load(filehandle)\n",
    "        \n",
    "    df_processed = pd.concat([df_processed, df_slice], ignore_index=True)\n",
    "    \n",
    "df_processed.info()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice using the filters\n",
    "df_ind = df_processed[(df_processed['poet_ok']==1)&(df_processed['tweet_ok']==1)\\\n",
    "                      &(df_processed['sent_ok']==1)&(df_processed['key_ok']==1)].copy()\n",
    "df_ind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index subset\n",
    "df_index = df_ind[['date','vad_compound']].copy()\n",
    "df_index.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dates ascending\n",
    "df_index = df_index.sort_values('date')\n",
    "df_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate by days\n",
    "df_index = df_index.set_index('date').groupby(pd.Grouper(freq='D')).mean()\n",
    "df_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip out the early NaN's\n",
    "df_index['2014-01-01':].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsi = df_index['2014-01-01':].copy()\n",
    "plt.plot(df_tsi.index, df_tsi['vad_compound'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(df_tsi['2019-01-01':].index, df_tsi['vad_compound']['2019-01-01':])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the indicator\n",
    "df_tsi.to_csv('tsi002.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This concludes the manual ETL process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
